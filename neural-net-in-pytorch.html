
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="https://sgugger.github.io/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://sgugger.github.io/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="https://sgugger.github.io/theme/font-awesome/css/font-awesome.min.css">





  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />


<meta name="author" content="Sylvain Gugger" />
<meta name="description" content="The theory is all really nice, but let's actually build a neural net and train it! We'll see how a simple neural net with one hidden layer can learn to recognize digits very efficiently." />
<meta name="keywords" content="neural net, pytorch">
<meta property="og:site_name" content="Another data science student's blog"/>
<meta property="og:title" content="Neural net in pytorch"/>
<meta property="og:description" content="The theory is all really nice, but let's actually build a neural net and train it! We'll see how a simple neural net with one hidden layer can learn to recognize digits very efficiently."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://sgugger.github.io/neural-net-in-pytorch.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-03-16 10:13:00-04:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://sgugger.github.io/author/sylvain-gugger.html">
<meta property="article:section" content="Deep learning"/>
<meta property="article:tag" content="neural net"/>
<meta property="article:tag" content="pytorch"/>
<meta property="og:image" content="https://sgugger.github.io/images/profile.png">

  <title>Another data science student's blog &ndash; Neural net in pytorch</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://sgugger.github.io">
        <img src="https://sgugger.github.io/images/profile.png" alt="Sylvain Gugger" title="Sylvain Gugger">
      </a>
      <h1><a href="https://sgugger.github.io">Sylvain Gugger</a></h1>


      <nav>
        <ul class="list">
          <li><a href="https://sgugger.github.io/pages/about-me.html#about-me">About me</a></li>

          <li><a href="http://fast.ai/" target="_blank">fast.ai</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/sylvain-gugger-74218b144/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://github.com/sgugger" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="https://sgugger.github.io">    Home
</a>

      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>
      <a href="/archives.html">Archives</a>


    </nav>

<article class="single">
  <header>
    <h1 id="neural-net-in-pytorch">Neural net in pytorch</h1>
    <p>
          Posted on Fri 16 March 2018 in <a href="https://sgugger.github.io/category/deep-learning.html">Deep learning</a>


    </p>
  </header>


  <div>
    <p>This article goes with <a class="reference external" href="http://www.google.com">this notebook</a> if you want to really do the experiment. In particular, I won't explain the specifics of getting the data and
preprocessing it here.</p>
<div class="section" id="pytorch">
<h2>Pytorch</h2>
<p>Pytorch is a Python library that provides all what is needed to implement deep-learning easily. In particular, it enables GPU-accelerated computations and provides automatic
differentiation. We have seen why the latter is useful in the previous article, and this the reason why we will never have to worry about calculating gradients
(unless we really want to dig into that).</p>
<p>But why GPUs? As we have seen, deep-learning is just a succession of linear operations with a few functions applied element-wise in between, and it happens that GPUs are really
good (and fast!) at those, because that's what is basically needed to decide which color should each pixel of the screen have when playing a game. Thanks to the gaming industry,
research on GPUs has made them extremely efficient, which is also why deep-learning has gained that much interest in the past few years. We can consider deeper network and train
them on much more data nowadays.</p>
<p>To use the full potential of this library, we're going to need a, preferably several, efficient GPU. A gaming computer can have one, but the best way is to rent some. Services
to rent GPUs by the hour have flourished and you can easily find some powerful virtual machines with 8 efficient GPUs for less than fifty cents an hour. I'm personally using
Paperspace at the moment.</p>
<p>I'm mostly using pytorch because the library of fast.ai is built on top of it, but I really like the way it uses Python functionalities (as we'll see, it makes good use of
Object Oriented Programming in Python) and the fact the gradients are dynamically computed. It's making the implementation of Recurrent Neural Networks a lot easier in my
opinion, but we'll see more of that later.</p>
</div>
<div class="section" id="mnist-dataset">
<h2>MNIST Dataset</h2>
<p>To have some data on which try our neural net, we will use the MNIST Dataset. It's a set of hand-written digits that contains 70,000 pictures with their labels. It is divided
in two parts, one training set with 60,000 digits (on which we will train our model) and 10,000 others that form the test. These were drawn by different people from the ones
in the first test, and by evaluating how well on this set, we will see how well it actually generalizes what it learned.</p>
<p>We'll skip the part as to how to get those sets and how to treat them since it's all shown in the notebook. Let's go to the part where we define our neural net instead. The
pictures we are given have a size of 28 by 28 pixels, each pixel having a value of 0 (white) to 1 (black), so that makes 784 inputs. For this simple model, we choose one
hidden layer of 100 neurons, and then an output size of 10 since we have ten different digits.</p>
<p>Why 10 and not 1? It's true that in this case we could have asked for just one output going from 0 to 9 (and there are ways to make sure it'd behave like this) but in
image classification problems, we often give as many outputs as they are classes to determine. What if our next problem is to say if the picture if of a dog, a cat, a frog or
a horse? One output won't really represent this, whereas four outputs will certainly help, each of them representing the probability it's in a given class.</p>
</div>
<div class="section" id="softmax">
<h2>Softmax</h2>
<p>When we have a classification problem and a neural network trying to solve it with <span class="math">\(N\)</span> outputs (the number of classes), we want those outputs to represent the probabilities
the input is in each of the classes. To make sure that's our final <span class="math">\(N\)</span> numbers are all positive and add up to one, we use the softmax activation for the last layer.</p>
<p>If <span class="math">\(z_{1},\dots,z_{N}\)</span> are the last activations given by our final linear layer, instead of pushing them through a ReLU or a sigmoid, we define the outputs
<span class="math">\(y_{1},\dots,y_{N}\)</span> by</p>
<div class="math">
\begin{equation*}
y_{i} = \frac{\mathrm{e}^{z_{i}}}{\mathrm{e}^{z_{1}} + \cdots + \mathrm{e}^{z_{N}}} = \frac{\mathrm{e}^{z_{i}}}{\sum_{k=1}^{N} \mathrm{e}^{z_{k}}}.
\end{equation*}
</div>
<p>As we take the exponentials of the <span class="math">\(z_{i}\)</span>, we are sure all of them are positive. Then since we divide by their sum, they must all add up to one, so softmax satisfies
all the perquisite we wanted for our final output.</p>
<p>One nice side effect (and which is the reason we chose the exponential) is that if one of the <span class="math">\(z_{i}\)</span> is slightly bigger than the other, its exponential will be a lot
bigger. This will have the effect that the corresponding <span class="math">\(y_{i}\)</span> will be close to 1, while the other <span class="math">\(y_{j}\)</span> are close to zero. Softmax is an activation that really
<em>wants</em> to pick one class over the other.</p>
<p>It's not essential, and a neural net could certainly learn with ReLU or sigmoid as its final activation function, but by using softmax we are making it easier for it to have
an output that is close to what we really want, so it will learn faster and generalize better.</p>
</div>
<div class="section" id="cross-entropy">
<h2>Cross Entropy</h2>
<p>To evaluate how badly our model is doing, we had seen the Mean Squared Error loss in the last article. When the output activation function is softmax or a sigmoid, another
function is usually used, called Cross Entropy Loss. If the correct class our model should pick is the <span class="math">\(i\)</span>-th, we define the loss as being <span class="math">\(-\ln(y_{i})\)</span> (when
the output is <span class="math">\((y_{1},\dots,y_{N}\)</span>.</p>
<p>Since all the <span class="math">\(y_{i}\)</span> are between 0 and 1, this loss is a positive number, and it vanishes when <span class="math">\(y_{i} = 1\)</span>. If <span class="math">\(y_{i}\)</span> is real low though (and we are doing
a mistake in choosing this class) it'll get particularly high.</p>
<p>If we had multiple correct answers (in a multi-classification problem) we would sum the <span class="math">\(-\ln(y_{i})\)</span> over all the correct classes <span class="math">\(i\)</span>.</p>
<p>Note that with the usual formulas, we have</p>
<div class="math">
\begin{equation*}
ln(y_{i}) = \ln \left ( \frac{\mathrm{e}^{z_{i}}}{\sum_{k=1}^{N} \mathrm{e}^{z_{k}}} \right ) = \ln(\mathrm{e}^{z_{i}}) - \ln \left (  \sum_{k=1}^{N} \mathrm{e}^{z_{k}}} \right ) = z_{i} - \ln \left (  \sum_{k=1}^{N} \mathrm{e}^{z_{k}}} \right ).
\end{equation*}
</div>
<p>so the derivative of the loss with respect to <span class="math">\(z_{i}\)</span> is</p>
<div class="math">
\begin{equation*}
\frac{\partial \hbox{loss}}{\partial z_{i}} = -1 + \frac\mathrm{e}^{z_{i}}}{\sum_{k=1}^{N} \mathrm{e}^{z_{k}}} = y_{i} - 1
\end{equation*}
</div>
<p>and the derivative of the loss with respect to <span class="math">\(z_{j}\)</span> with <span class="math">\(j \neq i\)</span> is</p>
<div class="math">
\begin{equation*}
\frac{\partial \hbox{loss}}{\partial z_{j}} = frac\mathrm{e}^{z_{j}}}{\sum_{k=1}^{N} \mathrm{e}^{z_{k}}} = y_{j}
\end{equation*}
</div>
<p>so it's always <span class="math">\(y_{j} - \hat{y_{j}}\)</span>, where <span class="math">\(\hat{y_{j}}\)</span> is the output we are supposed to obtain. This simplification makes it easier to compute the gradients, and
it also has the advantage of giving a higher gradient when the error is big, whereas with the MSE loss we'd end up with slower ones, hence learning more slowly.</p>
<p>In practice, pytorch implemented the computation of log softmax faster than softmax, and since we're using the log of the softmax in our loss function, we'll use
log softmax as the output activation function.</p>
</div>
<div class="section" id="writing-our-model">
<h2>Writing our model</h2>
<p>With all we have seen, we're ready to write our neural net in pytorch. To do that, we create a specific kind of nn.Module</p>
</div>
<script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://sgugger.github.io/tag/neural-net.html">neural net</a>
      <a href="https://sgugger.github.io/tag/pytorch.html">pytorch</a>
    </p>
  </div>




</article>

    <footer>
<p>&copy; Sylvain Gugger 2018</p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Another data science student's blog ",
  "url" : "https://sgugger.github.io",
  "image": "https://sgugger.github.io/images/profile.png",
  "description": ""
}
</script>
</body>
</html>