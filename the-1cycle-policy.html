
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/font-awesome.min.css">


    <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Another data science student's blog Atom">

    <link href="/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Another data science student's blog RSS">


  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />


<meta name="author" content="Sylvain Gugger" />
<meta name="description" content="Properly setting the hyper-parameters of a neural network can be challenging, fortunately, there are some recipe that can help." />
<meta name="keywords" content="Deep Learning, SGD, Learning Rate">
<meta property="og:site_name" content="Another data science student's blog"/>
<meta property="og:title" content="The 1cycle policy"/>
<meta property="og:description" content="Properly setting the hyper-parameters of a neural network can be challenging, fortunately, there are some recipe that can help."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/the-1cycle-policy.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-04-07 15:23:00-04:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/sylvain-gugger.html">
<meta property="article:section" content="Experiments"/>
<meta property="article:tag" content="Deep Learning"/>
<meta property="article:tag" content="SGD"/>
<meta property="article:tag" content="Learning Rate"/>
<meta property="og:image" content="/images/profile.png">

  <title>Another data science student's blog &ndash; The 1cycle policy</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://sgugger.github.io">
        <img src="/images/profile.png" alt="Sylvain Gugger" title="Sylvain Gugger">
      </a>
      <h1><a href="https://sgugger.github.io">Sylvain Gugger</a></h1>


      <nav>
        <ul class="list">
          <li><a href="/pages/about-me.html#about-me">About Me</a></li>

          <li><a href="http://fast.ai/" target="_blank">fast.ai</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/sylvain-gugger-74218b144/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://github.com/sgugger" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/GuggerSylvain" target="_blank"><i class="fa fa-twitter"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="https://sgugger.github.io">    Home
</a>

      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>
      <a href="/archives.html">Archives</a>

      <a href="/feeds/all.atom.xml">    Atom
</a>

      <a href="/feeds/all.rss.xml">    RSS
</a>
    </nav>

<article class="single">
  <header>
    <h1 id="the-1cycle-policy">The 1cycle policy</h1>
    <p>
          Posted on Sat 07 April 2018 in <a href="/category/experiments.html">Experiments</a>


    </p>
  </header>


  <div>
    <p>Here, we will dig into the <a class="reference external" href="https://arxiv.org/abs/1803.09820">first part</a> of Leslie Smith's work about setting hyper-parameters (namely learning rate, momentum and weight decay). In particular, his 1cycle policy
gives very fast results to train complex models. As an example, we'll see how it allows us to train a resnet-56 on cifar10 to the same or a better precision than the authors in
<a class="reference external" href="https://arxiv.org/abs/1512.03385">their original paper</a> but with far less iterations.</p>
<p>By training with high learning rates we can reach a model that gets <strong>93% accuracy in 70 epochs</strong> which is less than
7k iterations (as opposed to the 64k iterations which made roughly 360 epochs in the original paper).</p>
<p><a class="reference external" href="https://github.com/sgugger/Deep-Learning/blob/master/Cyclical%20LR%20and%20momentums.ipynb">This notebook</a> contains all the experiments.
They are done with the same data-augmentation as in this original paper with one minor tweak: we random flip the picture horizontally and a random crop after adding a padding of
4 pixels on each side. The minor tweak is that we don't color the padded pixels in black, but use a reflection padding, since it's the one implemented
in the fastai library. This is probably why we get slightly better results than Leslie when doing the experiments with the same hyper-parameters.</p>
<div class="section" id="using-high-learning-rates">
<h2>Using high learning rates</h2>
<p>We have already seen how to implement the <a class="reference external" href="/how-do-you-find-a-good-learning-rate.html">learning rate finder</a>. Begin to train the model while increasing the learning rate from
a very low to a very large one, stop when the loss starts to really get out of control. Plot the losses against the learning rates and pick a value a bit before the minimum,
where the loss still improves. Here for instance, anything between <span class="math">\(10^{-2}\)</span> and <span class="math">\(3 \times 10^{-2}\)</span> seems like a good idea.</p>
<img alt="An example of curve when finder the learning rate" class="align-center" src="../images/art2_courbe_lr.png" style="width: 400px;" />
<p>This was already an idea of the same author and he completes it in his <a class="reference external" href="https://arxiv.org/abs/1803.09820">last article</a> with a good approach to adopt during training.</p>
<p>He recommends to do a cycle with two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked
with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last
part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude.</p>
<img alt="Learning rates to use in a cycle" class="align-center" src="../images/art5_lr_schedule.png" style="width: 400px;" />
<p>The idea of starting slower isn't new: using a lower value to warm-up the training is often done, and this is exactly what the first part is achieving. Leslie doesn't recommend
to switch to a higher value directly, however, but to rather slowly go there linearly, and to take as much time going up as going down.</p>
<p>What he observed during his experiments is that the during the middle of the cycle, the high learning rates will act as regularization method, and keep the network from overfitting.
They will prevent the model to land in a steep area of the loss function, preferring to find a minimum that is flatter. He explained in <a class="reference external" href="https://arxiv.org/abs/1708.07120">this other paper</a> how he observed that by using this policy, approximates of the hessian were lower, indicating that the SGD was finding a wider flat area.</p>
<p>Then the last part of the training, with descending learning rates up until annihilation will allow us to go inside a steeper local minimum inside that smoother part. During the
par with high learning rates, we don't see substantial improvements in the loss or the accuracy, and the validation loss sometimes spikes very high, but we see all the benefits
of doing this when we finally lower the learning rates at the end.</p>
<img alt="Losses during a full cycle" class="align-center" src="../images/art5_losses.png" style="width: 400px;" />
<p>In this graph, the learning rate was rising from 0.08 to 0.8 between epochs 0 and 41, getting back to 0.08 between epochs 41 and 82 then going to one hundredth of 0.08 in the last
few epochs. We can see how the validation loss gets a little bit more volatile during the high learning rate part of the cycle (epochs 20 to 60 mostly) but the important part is
that on average, the distance between the training loss and the validation loss doesn't increase. We only really start to overfit at the end of the cycle, when the learning rate
gets annihilated.</p>
<p>Surprisingly, applying this policy even allows us to pick larger maximum learning rates, closer to the minimum of the plot we draw when using the learning rate finder. Those
trainings are a bit more dangerous in the sense that the loss can go too far away and make the whole thing diverge. In those cases, it can be worth to try with a longer cycle before going
to a slower learning rate, since a long warm-up seems to help.</p>
<img alt="Losses during a full cycle" class="align-center" src="../images/art5_superconvergence.png" style="width: 400px;" />
<p>In this graph, the learning rate was rising from 0.15 to 3 between epochs 0 and 22.5, getting back to 0.15 between epochs 22.5 and 45 then going to one hundredth of 0.15 in the last
few epochs. With very high learning rates, we get to learn faster <strong>and</strong> prevent overfitting. The difference between the validation loss and the training loss stays extremely low
up until we annihilate the learning rates. This is the phenomenon Leslie Smith describes as super convergence.</p>
<p>With this technique, we can train a resnet-56 to have 92.3% accuracy on cifar10 in barely 50 epochs. Going to a cycle of 70 epochs gets us at 93% accuracy.</p>
<p>By opposition, a smaller cycle followed by a longer annihilation will result in something like this:</p>
<img alt="An example of overfitting" class="align-center" src="../images/art5_overfitting.png" style="width: 400px;" />
<p>Here our two steps end at epoch 42 and the rest of the training is spent with a learning rate slowly decreasing. The validation loss stops decreasing causing bigger and bigger
overfitting, and the accuracy barely gets up.</p>
</div>
<div class="section" id="cyclical-momentum">
<h2>Cyclical momentum</h2>
<p>To accompany the movement toward larger learning rates, Leslie found in his experiments that decreasing the momentum led to better results. This supports the intuition that in
that part of the training, we want the SGD to quickly go in new directions to find a flatter area, so the new gradients need to be given more weight. In practice, he recommends
to pick two values likes 0.85 and 0.95, and decrease from the higher one to the lower one when we increase the learning rate, then go back to the higher momentum as the learning
rate goes down.</p>
<img alt="Learning rate and momentum schedule" class="align-center" src="../images/art5_full_schedule.png" style="width: 600px;" />
<p>According to Leslie, the exact best value of momentum chosen during the whole training can give us the same final results, but using cyclical momentums removes the hassle of trying multiple values
and running several full cycles, losing precious time.</p>
<p>Even if using cyclical momentum always gave slightly better results, I didn't find the same gap as in the paper between using a constant momentum and cyclical ones.</p>
</div>
<div class="section" id="all-the-other-parameters-matter">
<h2>All the other parameters matter</h2>
<p>The way we tune all the other hyper-parameters of the model will impact the best learning rate. That's why when we run the Learning Rate Finder, it's very important to use it
with the exact same conditions as during our training. For instance different batch sizes or weight decays will impact the results:</p>
<img alt="LR Finder for various weight decay values" class="align-center" src="../images/art5_wds.png" style="width: 400px;" />
<p>This can be useful to set some hyper-parameters. For instance, with weight decay, Leslie's advice is to run the learning rate finder
for a few values of weight decay, and pick the largest one that will still let us train at a high maximum learning rate. This is how we can come up with the <span class="math">\(10^{-4}\)</span> used
in our experiments.</p>
<p>In his opinion, the batch size should be set to the highest possible value to fit in the available memory. Then the other hyper-parameters we may have (dropout for instance) can
be tuned the same way as weight decay, or just by trying on a cycle and see the results they give. The only thing is to never forget to re-run the Learning Rate Finder, especially
when deciding to pick a strategy with an aggressive learning rate close to the maximum possible value.</p>
<p>Training with the 1cycle policy at high learning rates is a method of regularization in itself, so we shouldn't be surprised if we have to reduce the other forms of regularization
we were previously using when we put it in place. It will however be more efficient, since we can train for a long time at large learning rates.</p>
</div>
<script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/deep-learning.html">Deep Learning</a>
      <a href="/tag/sgd.html">SGD</a>
      <a href="/tag/learning-rate.html">Learning Rate</a>
    </p>
  </div>




</article>

    <footer>
<p>&copy; Sylvain Gugger 2018</p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Another data science student's blog ",
  "url" : "",
  "image": "/images/profile.png",
  "description": ""
}
</script>
</body>
</html>